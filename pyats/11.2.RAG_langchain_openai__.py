import os
import shutil
from pathlib import Path
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

# ğŸ”¹ Ensure OpenAI API key is set
if not os.getenv("OPENAI_API_KEY"):
    raise EnvironmentError("âŒ OPENAI_API_KEY environment variable not set!")

# ğŸ”¹ Set paths
base_dir = Path(__file__).parent
text_path = base_dir / "logging.txt"
chroma_dir = base_dir / "chroma_temp"

# ğŸ”¹ Check file existence
print(f"ğŸ“ Working directory: {base_dir}")
print(f"ğŸ“„ Log file exists: {text_path.exists()}")

if not text_path.exists():
    raise FileNotFoundError(f"âŒ File not found: {text_path}")

# 1ï¸âƒ£ Load documents
loader = TextLoader(str(text_path))
documents = loader.load()
print(f"âœ… Loaded {len(documents)} document(s)")

# 2ï¸âƒ£ Split into smaller chunks
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_documents(documents)
print(f"âœ… Split into {len(docs)} chunks")

# ğŸ”¹ Show sample text
print("ğŸ” Sample text chunk:\n", docs[0].page_content[:250], "\n---")

# 3ï¸âƒ£ Reset Chroma vector database each run
shutil.rmtree(chroma_dir, ignore_errors=True)
chroma_dir.mkdir(exist_ok=True)

# 4ï¸âƒ£ Create embeddings and vectorstore
embeddings = OpenAIEmbeddings()
db = Chroma.from_documents(docs, embeddings, persist_directory=str(chroma_dir))

# 5ï¸âƒ£ Create retriever
retriever = db.as_retriever(search_kwargs={"k": 4})

# 6ï¸âƒ£ Initialize LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 7ï¸âƒ£ Create RetrievalQA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)

# 8ï¸âƒ£ Ask question
query = "What are the most important messages in the log file in summary?"
print(f"\nğŸ§  Question: {query}")

result = qa_chain.invoke({"query": query})
print(f"ğŸ’¬ Answer:\n{result['result']}")
